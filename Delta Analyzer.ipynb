{"cells":[{"cell_type":"code","execution_count":null,"id":"0d14568e-e1df-4643-97eb-7c08cab2b863","metadata":{"microsoft":{"language":"scala"}},"outputs":[],"source":["%%spark\n","//Imports\n","    import org.apache.spark.sql.delta.DeltaLog\n","    import org.apache.hadoop.conf.Configuration\n","    import org.apache.hadoop.fs.Path\n","    import org.apache.parquet.hadoop.ParquetFileReader\n","    import org.apache.parquet.hadoop.metadata.ParquetMetadata\n","    import org.apache.parquet.hadoop.metadata.FileMetaData\n","    import org.apache.parquet.hadoop.metadata.BlockMetaData\n","    import org.apache.spark.sql.Row\n","    import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType, FloatType}\n","    import scala.collection.mutable.ListBuffer\n","    import org.apache.spark.sql.DataFrame\n","    import java.time.LocalDateTime\n","    import org.apache.spark.sql.functions.lit\n","\n","\n","/*****************************************************************************************************\n","    Instructions:\n","    - Connect Notebook to Lakehouse (does not have to be in same Workspace)\n","    - Update deltaTable parameter immedately below instructions with name of table to analyze\n","    - Review other parameters (append vs overwrite) \n","    - Run\n","    - Review four output tables that have \"zz_DeltaAnalyzerOutput\"\n","\n","    zz_DeltaAnalyzerOutput_parquetFiles\n","        This table has one row per Parquet file\n","        Ideally, there should not be thousands of these\n","        This table only uses parquet file metadata and should be quick to populate\n","\n","    zz_DeltaAnalyzerOutput_rowRowgroups\n","        This table has one row per rowgroup and shows rowgroups for every parquet file\n","        Look for the number of rows per rowgroup.  Ideally this should be 1M to 16M rows (higher the better)\n","        This table only uses parquet file metadata and should be quick to populate\n","\n","    zz_DeltaAnalyzerOutput_columnChunks\n","        One row per column/chunk within rowgroups\n","        Large number of output and has much more detail about dictionaries and compression\n","        This table only uses parquet file metadata and should be quick to populate\n","    \n","    zz_DeltaAnalyzerOutput_columns\n","        One row per column of the table\n","        Look to see how many unique values per column.  If using floating point, consider modifying parquet file to use DECIMAL(17,4)\n","        This table runs a compute query against the Detla table so may take time depending on size of Delta table\n","\n","    Run \n","    %%sql\n","    OPTIMIZE tablename vorder \n","\n","\n","    Footnote:\n","        Useful doc\n","        https://www.javadoc.io/doc/org.apache.parquet/parquet-hadoop/latest/index.html\n","\n","*****************************************************************************************************/\n","\n","//Parameters\n","val deltaTable: String = \"fact_myevents_2bln\"   \n","\n","val timeStamp = LocalDateTime.now().toString\n","val overwriteOrAppend: String = \"Append\"     // \"Append\" or \"Overwrite\"\n","var printToScreen: Boolean = false              // true or false\n","\n","//Code\n","\n","    val LakehouseID: String = \"dc8e7126-8b8d-4065-a2c8-5d266e25e296\" // sc.hadoopConfiguration.get(\"trident.lakehouse.id\")\n","    val defaultFS: String = sc.hadoopConfiguration.get(\"fs.defaultFS\")\n","    val deltaTablePath: String = s\"${defaultFS}/${LakehouseID}/Tables/${deltaTable}/\"\n","\n","    val deltaLog: DeltaLog = DeltaLog.forTable(spark, deltaTablePath)\n","    val formatter = java.text.NumberFormat.getInstance\n","\n","    deltaLog.update()\n","    println(s\"Files: ${deltaLog.snapshot.allFiles.count}\")\n","\n","    var parquetFiles: ListBuffer[(String,String,String,String,String,Long,String)] = ListBuffer()\n","    var rowGroups: ListBuffer[(String,String,String,Int,Int,Long,Long,Long,Float)] = ListBuffer()\n","    var columnChunks: ListBuffer[(String,String,String,Int,String,String,String,String,Long,Long,Long,String,Long,String)] = ListBuffer()\n","\n","    val totalNumberRowGroups: Int = deltaLog.snapshot.allFiles.collect().map{ file => \n","        val path: String = file.path    \n","        val configuration = new Configuration()\n","\n","        val parquetFileReader: ParquetFileReader = ParquetFileReader.open(configuration, new Path(deltaTablePath + path))\n","\n","        val rowGroupMeta = parquetFileReader.getRowGroups() //java.util.ArrayList\n","        val parquetFileMetaData: FileMetaData = parquetFileReader.getFileMetaData()\n","        val parquetFileMetaDataKeyValues = parquetFileMetaData.getKeyValueMetaData() // Array\n","\n","        val rowGroupSize: Int = rowGroupMeta.size\n","\n","        parquetFiles.append(\n","            (\n","                deltaTable ,\n","                timeStamp,\n","                path , \n","                parquetFileMetaDataKeyValues.get(\"com.microsoft.parquet.vorder.enabled\"),\n","                parquetFileMetaDataKeyValues.get(\"com.microsoft.parquet.vorder.level\"),\n","                rowGroupSize,\n","                parquetFileMetaData.getCreatedBy\n","            )\n","        )\n","\n","        for(rowGroupNumber <- 0 to rowGroupSize -1)\n","        {\n","            val rowGroup: BlockMetaData = rowGroupMeta.get(rowGroupNumber)\n","\n","            //println(rowGroup.getColumns.size)\n","            //val rowGroupColumns: List[org.apache.parquet.hadoop.metadata.ColumnChunkMetaData] = rowGroup.getColumns\n","            val rowGroupColumns = rowGroup.getColumns\n","\n","            for(columnChunkID <- 0 to rowGroupColumns.size-1)\n","                {\n","                val columnStat = rowGroupColumns.get(columnChunkID)\n","\n","                columnChunks.append(\n","                    (\n","                        deltaTable,\n","                        timeStamp,\n","                        path,\n","                        rowGroupNumber,\n","                        columnStat.getPath().toString,\n","                        columnStat.getCodec().toString,\n","                        columnStat.getPrimitiveType().toString,\n","                        columnStat.getStatistics().toString().replace(\"\\\"\",\"'\"), // need to convert double quotes to single quotes for Power BI\n","                        columnStat.getTotalSize(),\n","                        columnStat.getTotalUncompressedSize(),\n","                        columnStat.getValueCount(),\n","                        columnStat.hasDictionaryPage().toString,\n","                        columnStat.getDictionaryPageOffset(),\n","                        columnStat.getEncodings().toString                      \n","                    )\n","                )\n","            }\n","\n","            val compressedPercent: Double = rowGroup.getCompressedSize.toFloat / rowGroup.getTotalByteSize * 100\n","            rowGroups.append(\n","                (\n","                    deltaTable,\n","                    timeStamp,\n","                    path, \n","                    rowGroupNumber,\n","                    rowGroupSize ,\n","                    rowGroup.getRowCount,\n","                    rowGroup.getCompressedSize,\n","                    rowGroup.getTotalByteSize,\n","                    rowGroup.getCompressedSize.toFloat / rowGroup.getTotalByteSize                \n","                )\n","            )\n","        }\n","        rowGroupSize\n","    }.sum\n","\n","    //println(\"Total number of row groups: \" + totalNumberRowGroups)\n","\n","    // Column Names cannot have spaces!!\n","    var parquetFilesDF: DataFrame = parquetFiles.toDF(\"TableName\",\"Timestamp\",\"Filename\",\"vorder_enabled\",\"vorder_level\",\"RowGroups\",\"CreatedBy\")\n","    var rowGroupsDF: DataFrame = rowGroups.toDF(\"TableName\",\"Timestamp\",\"Filename\",\"RowGroupID\",\"TotalFileRowGroups\",\"Rowcount\",\"CompressedSize\",\"UncompressedSize\",\"CompressionRatio\")\n","    val columnChunksDF: DataFrame = columnChunks.toDF(\"TableName\",\"Timestamp\",\"Filename\",\"ColumnChunkID\",\"Path\",\"Codec\",\"PrimativeType\",\"Statistics\",\"TotalSize\",\"TotalUncompressedSize\",\"ValueCount\",\"HasDict\",\"DictOffset\",\"Encodings\"   )\n","\n","    /********************************************************************************************************************\n","        Create DF for one row per column\n","    ********************************************************************************************************************/\n","\n","        var columns: ListBuffer[(String,String,String,Long,String,Long,Long,Long,Long,Double,Double)] = ListBuffer()\n","        val columnList: Array[String] = spark.table(deltaTable).columns\n","\n","        val totalRows = rowGroupsDF.agg(sum(\"Rowcount\")).first().getLong(0)\n","        val tableSize = rowGroupsDF.agg(sum(\"CompressedSize\")).first().getLong(0)\n","        val totalRowGroups = parquetFilesDF.agg(sum(\"RowGroups\")).first().getLong(0)\n","\n","        if(totalRows<=1000000)\n","        {\n","            /* Using single query for all columns */\n","            var sql: String = \"select 1 as dummy\"\n","            for(column <- columnList) \n","                {\n","                sql+=(s\", count(distinct ${column}) as ${column}\")\n","                }\n","            sql +=s\" from ${deltaTable}\"\n","            val distinctDFblock: DataFrame = spark.sql(sql)\n","        \n","            for(column <- columnList) \n","            {\n","                val filtercondition: String = s\"Path = '[${column.toString}]'\"\n","                val distinctCount = distinctDFblock.select(col(column).cast(\"long\")).first().getLong(0)\n","                val primativeType= columnChunksDF.filter(filtercondition).select(col(\"PrimativeType\")).first().getString(0)\n","                val columnSize = columnChunksDF.filter(filtercondition).agg(sum(\"TotalSize\")).first().getLong(0)\n","                val columnSizeUncompressed = columnChunksDF.filter(filtercondition).agg(sum(\"TotalUncompressedSize\")).first().getLong(0)\n","\n","                columns.append(\n","                        (\n","                        deltaTable,\n","                        timeStamp,\n","                        column ,\n","                        distinctCount,\n","                        primativeType,\n","                        columnSize,\n","                        columnSizeUncompressed,\n","                        totalRows ,\n","                        tableSize ,\n","                        distinctCount.toDouble/totalRows*100.0,\n","                        columnSize.toDouble/tableSize*100.0\n","                        )\n","                    )\n","            }\n","\n","        } else {\n","             var NotTheseColumns = List(\"requestPathUri\",\"clientRequestId\",\"fileSystemID\")\n","            for(column <- columnList) \n","                {\n","                val filtercondition: String = s\"Path = '[${column.toString}]'\"\n","\n","                val primativeType= columnChunksDF.filter(filtercondition).select(col(\"PrimativeType\")).first().getString(0)\n","                val columnSize = columnChunksDF.filter(filtercondition).agg(sum(\"TotalSize\")).first().getLong(0)\n","                val columnSizeUncompressed = columnChunksDF.filter(filtercondition).agg(sum(\"TotalUncompressedSize\")).first().getLong(0)\n","\n","\n","                var distinctCount: Long = 0;\n","                if(!NotTheseColumns.contains(column))\n","                {\n","                    var sql:String = \"\"\n","\n","                   if(totalRows<10000000)\n","                    {\n","                        println(\"Running Precise DCOUNT on \" + column) \n","                        sql = s\"select count(distinct ${column}) as ${column} from ${deltaTable}\"\n","                        val distinctDF: DataFrame  = spark.sql(sql)\n","                        distinctCount = distinctDF.select(col(column).cast(\"long\")).first().getLong(0)\n","                    } else {\n","                        println(\"Running Approx DCOUNT on \" + column) \n","                        sql = s\"select approx_count_distinct(${column}) as ${column} from ${deltaTable}\"\n","                        val distinctDF: DataFrame  = spark.sql(sql)\n","                        distinctCount = distinctDF.select(col(column).cast(\"long\")).first().getLong(0)       \n","                    }\n","\n","    \n","                } else {\n","                    println(\"Skipping \" + column) \n","                }\n","\n","                columns.append(\n","                        (\n","                        deltaTable,\n","                        timeStamp,\n","                        column ,\n","                        distinctCount,\n","                        primativeType,\n","                        columnSize,\n","                        columnSizeUncompressed,\n","                        totalRows ,\n","                        tableSize ,\n","                        distinctCount.toDouble/totalRows*100.0,\n","                        columnSize.toDouble/tableSize*100.0\n","                        )\n","                    )\n","                }\n","             }\n","        val columnsDF: DataFrame = columns.toDF(\"TableName\",\"Timestamp\",\"ColumnName\",\"DistinctCount\",\"PrimitiveType\",\"ColumnSize\",\"ColumnSizeUncompressed\",\"TotalRows\",\"TableSize\",\"CardinalityOfTotalRows\",\"SizePercentOfTable\")\n","\n","    parquetFilesDF = parquetFilesDF\n","                    .withColumn(\"TotalTableRows\",lit(totalRows))\n","                    .withColumn(\"TotalTableRowGroups\",lit(totalRowGroups))\n","    rowGroupsDF = rowGroupsDF\n","                    .withColumn(\"TotalTableRows\",lit(totalRows))\n","                    .withColumn(\"RatioOfTotalTableRows\", col(\"Rowcount\")*100.0 / lit(totalRows))\n","                    .withColumn(\"TotalTableRowGroups\",lit(totalRowGroups))\n","\n","\n","   // Display Dataframes to screen\n","        //if(printToScreen)\n","            display(parquetFilesDF)\n","            display(rowGroupsDF)\n","            display(columnChunksDF)\n","            display(columnsDF)\n","\n","    // Save Dataframe to Lakehouse files\n","    parquetFilesDF\n","        .write\n","        .mode(overwriteOrAppend)\n","        .option(\"header\", \"true\")\n","        .option(\"overwriteSchema\", \"true\")\n","        .format(\"delta\")\n","        .save(\"./Tables/zz_DeltaAnalyzerOutput_parquetFiles\")\n","\n","    rowGroupsDF\n","        .write\n","        .mode(overwriteOrAppend)\n","        .option(\"header\", \"true\")\n","        .option(\"overwriteSchema\", \"true\")\n","        .format(\"delta\")\n","        .save(\"./Tables/zz_DeltaAnalyzerOutput_rowGroups\")\n","\n","    columnChunksDF\n","        .write\n","        .mode(overwriteOrAppend)\n","        .option(\"header\", \"true\")\n","        .option(\"overwriteSchema\", \"true\")\n","        .format(\"delta\")\n","        .save(\"./Tables/zz_DeltaAnalyzerOutput_columnChunks\")\n","\n","    columnsDF\n","        .write\n","        .mode(overwriteOrAppend)\n","        .option(\"header\", \"true\")\n","        .option(\"overwriteSchema\", \"true\")\n","        .format(\"delta\")\n","        .save(\"./Tables/zz_DeltaAnalyzerOutput_columns\")\n"]}],"metadata":{"kernelspec":{"display_name":"python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"host":{"trident":{"lakehouse":{"default_lakehouse":"eb69df7f-ffd3-4b6f-842c-f62eaa33f107","known_lakehouses":"[{\"id\":\"eb69df7f-ffd3-4b6f-842c-f62eaa33f107\"}]"}}},"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"notebook_environment":{},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{},"enableDebugMode":false}},"synapse_widget":{"state":{},"version":"0.1"},"trident":{"lakehouse":{"default_lakehouse":"eb69df7f-ffd3-4b6f-842c-f62eaa33f107","default_lakehouse_name":"Big_Demo_DB","default_lakehouse_workspace_id":"ec3384e9-4d95-435c-b19b-816b0ebd0fb1","known_lakehouses":[{"id":"eb69df7f-ffd3-4b6f-842c-f62eaa33f107"}]}},"widgets":{}},"nbformat":4,"nbformat_minor":5}
